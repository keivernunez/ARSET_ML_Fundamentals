{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keivernunez/ARSET_ML_Fundamentals/blob/main/Clase5/Note3_de_4_Spacy_Transfrormer_b%C3%A1sico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pQ72Tw6BZgy"
      },
      "source": [
        "# Modelos Transformer para Traduccion Automatica\n",
        "\n",
        "En este cuaderno se explora la implementacion de un modelo Transformer para la traduccion automatica de portugues a ingles usando SpaCy con integracion de transformers de Hugging Face. Se utiliza un conjunto de datos paralelo y se aprovecha la eficiencia de SpaCy para inferencia rapida.\n",
        "\n",
        "## Objetivos de Aprendizaje\n",
        "- Comprender la arquitectura Transformer y sus ventajas sobre modelos recurrentes.\n",
        "- Usar SpaCy con transformers preentrenados para tareas de traduccion.\n",
        "- Preprocesar datos de texto para modelado secuencia a secuencia.\n",
        "- Realizar inferencia para generar traducciones con mayor velocidad.\n",
        "\n",
        "**Nota:** Este cuaderno esta disenado con fines didacticos. Cada seccion incluye explicaciones detalladas, fragmentos de codigo comentados y visualizaciones para facilitar la comprension. Basado en el trabajo seminal \"Attention is All You Need\" (Vaswani et al., 2017), se incorporan practicas recomendadas para el procesamiento de lenguaje natural. Se refactoriza para usar SpaCy, que es mas rapido para inferencia.\n",
        "\n",
        "**Conceptos Clave a Recordar:**\n",
        "- **Transformer:** Arquitectura basada en atencion que procesa secuencias en paralelo, superando limitaciones de modelos RNN/LSTM.\n",
        "- **Atencion Multi-Cabeza:** Mecanismo que permite capturar multiples relaciones semanticas simultaneamente.\n",
        "- **SpaCy con Transformers:** Proporciona inferencia rapida y eficiente para modelos preentrenados.\n",
        "\n",
        "**Pregunta para Reflexion:** Â¿Por que la paralelizacion en Transformers mejora la eficiencia computacional en comparacion con modelos recurrentes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlWx5XEfBZg2"
      },
      "source": [
        "## 1. Importacion de Bibliotecas\n",
        "\n",
        "**Explicacion:** Se importan bibliotecas esenciales para el manejo de datos y modelado con SpaCy. SpaCy facilita la integracion de transformers para NLP eficiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf-Id4oIaKQi"
      },
      "outputs": [],
      "source": [
        "# Se instalan las librerÃ­as actualizadas - Reiniciar el runtime si es necesario\n",
        "\n",
        "%pip install -U pip wheel setuptools -q\n",
        "%pip install spacy spacy-transformers transformers sacremoses -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3krNDunBZg3"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import spacy_transformers\n",
        "from transformers import pipeline\n",
        "import tensorflow_datasets as tfds\n",
        "from IPython.display import Markdown, display\n",
        "import torch\n",
        "mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "print(\"âœ… Bibliotecas importadas con exito.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸš« Suprimir todas las advertencias y registros\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" # Ocultar informaciÃ³n/advertencias/errores de TensorFlow\n",
        "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\" # Ocultar registros de Transformers\n",
        "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Evitar advertencia de paralelismo del tokenizador\n",
        "# Opcional: reducir ruido de stderr en Python\n",
        "import logging\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"huggingface_hub\").setLevel(logging.ERROR)\n",
        "print(\"âœ… Todas las advertencias y registros suprimidos.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNLvie63f2OT",
        "outputId": "1cd00d9d-4827-423e-fc15-2a6ab48c30cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Todas las advertencias y registros suprimidos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "## Configuracion de GPU\n",
        "\n",
        "**Explicacion:** Configuramos el entorno para utilizar GPU si esta disponible, mejorando los tiempos de inferencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZYqILmsblq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f64908-ed2f-4b27-9400-20614629b6a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: CPU\n"
          ]
        }
      ],
      "source": [
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f'Using device: {\"GPU\" if device == 0 else \"CPU\"}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGSnZTYEBZg5"
      },
      "source": [
        "## 2. Carga del Conjunto de Datos\n",
        "\n",
        "**Explicacion:** Se carga el conjunto TED Talks para traduccion portugues-ingles. Este corpus paralelo es ideal para tareas seq2seq.\n",
        "\n",
        "**Pregunta para Reflexion:** Â¿Que desafios presenta un corpus multilingue en terminos de preprocesamiento?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq-nMO4tBZg5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "from transformers import pipeline\n",
        "import spacy\n",
        "\n",
        "device = 0  # GPU si existe, -1 para CPU\n",
        "\n",
        "# Cargar dataset de ejemplo portuguÃ©s â†’ inglÃ©s\n",
        "examples, metadata = tfds.load(\n",
        "    \"ted_hrlr_translate/pt_to_en\",\n",
        "    with_info=True,\n",
        "    as_supervised=True\n",
        ")\n",
        "train_examples, val_examples = examples[\"train\"], examples[\"validation\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRCoOjTgBZg7"
      },
      "source": [
        "## 3. Configuracion del Modelo con SpaCy\n",
        "\n",
        "**Explicacion:** Usamos SpaCy con un transformer preentrenado para traduccion. Cargamos un modelo de Hugging Face para PT-EN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5i6mZNGBZg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af604a46-f7cd-4527-f60a-9f696fba819f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¹ PortuguÃ©s: e quando melhoramos a procura , tiramos a Ãºnica vantagem da impressÃ£o , que Ã© a serendipidade .\n",
            "ðŸ”¸ TraducciÃ³n predicha: and when we improve the search , we take the only advantage of printing , which is serendipability .\n",
            "âœ… TraducciÃ³n real: and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
            "\n",
            "ðŸ”¹ PortuguÃ©s: mas e se estes fatores fossem ativos ?\n",
            "ðŸ”¸ TraducciÃ³n predicha: But what if these factors were active?\n",
            "âœ… TraducciÃ³n real: but what if it were active ?\n",
            "\n",
            "ðŸ”¹ PortuguÃ©s: mas eles nÃ£o tinham a curiosidade de me testar .\n",
            "ðŸ”¸ TraducciÃ³n predicha: but they didn't have the curiosity to test me.\n",
            "âœ… TraducciÃ³n real: but they did n't test for curiosity .\n"
          ]
        }
      ],
      "source": [
        "# Cargar modelo de traduccion con Hugging Face via transformers (integrable con SpaCy)\n",
        "translator = pipeline(\n",
        "    \"translation\",\n",
        "    model=\"Helsinki-NLP/opus-mt-mul-en\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# IntegraciÃ³n opcional con SpaCy\n",
        "nlp = spacy.blank(\"pt\")\n",
        "\n",
        "if not spacy.tokens.Doc.has_extension(\"translation\"):\n",
        "    spacy.tokens.Doc.set_extension(\"translation\", default=None)\n",
        "\n",
        "@nlp.component(\"translator\")\n",
        "def translate_component(doc):\n",
        "    translation = translator(doc.text)[0][\"translation_text\"]\n",
        "    doc._.translation = translation\n",
        "    return doc\n",
        "\n",
        "nlp.add_pipe(\"translator\")\n",
        "\n",
        "# Ejemplo de uso con el dataset TFDS\n",
        "for pt_text, en_text in train_examples.take(3):\n",
        "    pt_text = pt_text.numpy().decode(\"utf-8\")\n",
        "    en_text = en_text.numpy().decode(\"utf-8\")\n",
        "    doc = nlp(pt_text)\n",
        "    print(f\"\\nðŸ”¹ PortuguÃ©s: {pt_text}\")\n",
        "    print(f\"ðŸ”¸ TraducciÃ³n predicha: {doc._.translation}\")\n",
        "    print(f\"âœ… TraducciÃ³n real: {en_text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck93BA8sBZg8"
      },
      "source": [
        "## 4. Preparacion de Datos e Inferencia\n",
        "\n",
        "**Explicacion:** Procesamos ejemplos para inferencia rapida con SpaCy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3HTNJshBZg8"
      },
      "outputs": [],
      "source": [
        "# Ejemplo de inferencia\n",
        "for pt, en in val_examples.take(5):\n",
        "    doc = nlp(pt.numpy().decode('utf-8'))\n",
        "    print(f'Original (PT): {pt.numpy().decode(\"utf-8\")}')\n",
        "    print(f'Traduccion: {doc._.translation}')\n",
        "    print(f'Referencia (EN): {en.numpy().decode(\"utf-8\")}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmoINgz9BZhD"
      },
      "source": [
        "## 5. Evaluacion\n",
        "\n",
        "**Explicacion:** Evaluar la precision de traduccion usando metricas como BLEU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gBLcck8dPly",
        "outputId": "1cccf316-1863-4601-c0c0-4593adb7a1a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU promedio: 0.20024389941688392\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Evaluar en validacion\n",
        "bleu_scores = []\n",
        "for pt, en in val_examples.take(100):\n",
        "    doc = nlp(pt.numpy().decode('utf-8'))\n",
        "    pred = doc._.translation.split()\n",
        "    ref = [en.numpy().decode('utf-8').split()]\n",
        "    bleu_scores.append(sentence_bleu(ref, pred))\n",
        "\n",
        "print(f'BLEU promedio: {np.mean(bleu_scores)}')"
      ]
    }
  ]
}